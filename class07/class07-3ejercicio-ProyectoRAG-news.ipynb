{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSUoHE5d2X2T"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Universidad EAFIT \n",
        "# 2025-2\n",
        "# SI7016 - NLP - Lecture 07\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Construir un sistema RAG que permita buscar información en una base de datos real de noticias, recuperando los artículos más relevantes y generando respuestas usando GPT-4.\n",
        "\n",
        "* Usa LangChain y FAISS para la recuperación.\n",
        "* Procesa un dataset real de noticias.\n",
        "* Genera respuestas basadas en noticias relevantes.\n",
        "* Evalúa la precisión con métricas como Recall@K y ROUGE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Instalación de dependencias\n",
        "%pip install chromadb langchain langchain_community openai pandas tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 - Descarga y Preprocesamiento del Dataset\n",
        "\n",
        "Usaremos el dataset \"News Category Dataset\" de Kaggle, que contiene noticias reales.\n",
        "\n",
        "Descarga el dataset desde Kaggle:\n",
        "\n",
        "https://www.kaggle.com/datasets/rmisra/news-category-dataset?resource=download\n",
        "\n",
        "descarga el archivo: News_Category_Dataset_v3.json.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cargar y Preprocesar el Dataset\n",
        "\n",
        "El dataset de noticias viene en formato JSON, por lo que lo cargaremos y seleccionaremos las columnas necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Cargar el dataset de noticias\n",
        "file_path = \"News_Category_Dataset_v3.json\"  # Asegúrate de cambiar esto si el nombre del archivo es diferente\n",
        "\n",
        "# Leer el archivo JSON línea por línea\n",
        "data = []\n",
        "with open(file_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Convertir a un DataFrame de pandas\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Seleccionar columnas relevantes\n",
        "df = df[['headline', 'short_description', 'category']]\n",
        "df = df.dropna()  # Eliminar filas con valores nulos\n",
        "\n",
        "# Crear una columna combinada con el título y la descripción para indexación\n",
        "df[\"content\"] = df[\"headline\"] + \". \" + df[\"short_description\"]\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# volverlo más pequeño para poderlo procesar en tiempos razonables de clase y en una laptop\n",
        "df = df[:10000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.describe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Crear la Base de Datos Vectorial en ChromaDB\n",
        "\n",
        "Usamos ChromaDB para almacenar y recuperar embeddings de noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Inicializar ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_news_db\")\n",
        "\n",
        "# Crear una colección en ChromaDB\n",
        "collection = chroma_client.get_or_create_collection(name=\"news_articles\")\n",
        "\n",
        "# Generar embeddings para cada noticia y almacenarlas en la base de datos\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "i=0\n",
        "# en una laptop 16 GB RAM Intel i5, 35seg por cada 100 registros, \n",
        "# cargar 10.000 registros, toma aprox = 1 hora\n",
        "for idx, row in df.iterrows():\n",
        "    if (i%100==0):\n",
        "        print(i)\n",
        "    i=i+1\n",
        "    collection.add(\n",
        "        ids=[str(idx)],\n",
        "        documents=[row[\"content\"]],\n",
        "        metadatas=[{\"category\": row[\"category\"], \"headline\": row[\"headline\"]}]\n",
        "    )\n",
        "\n",
        "print(\"Base de datos de noticias creada con éxito.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inicializamos ChromaDB en modo persistente.\n",
        "Generamos embeddings con OpenAI y almacenamos los textos en ChromaDB.\n",
        "Guardamos metadatos como categoría y título para mejorar la interpretación de los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Búsqueda Semántica con Recuperación de Noticias\n",
        "\n",
        "Ahora creamos una función para buscar las noticias más relevantes usando búsqueda vectorial en ChromaDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_news(query, top_k=3):\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=top_k\n",
        "    )\n",
        "    return results[\"documents\"][0], results[\"metadatas\"][0]\n",
        "\n",
        "# Prueba de búsqueda\n",
        "query = \"News about Trump, white house\"\n",
        "retrieved_docs, metadata = search_news(query)\n",
        "\n",
        "print(\"\\nNoticias recuperadas:\")\n",
        "for i in range(len(retrieved_docs)):\n",
        "    print(f\"\\n{metadata[i]['headline']}\")\n",
        "    print(f\"\\n{retrieved_docs[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "search_news() consulta ChromaDB para recuperar los artículos más relevantes.\n",
        "Ejemplo de consulta: Buscamos noticias sobre \"elecciones en EE.UU.\" y mostramos los títulos y descripciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Generación de Respuestas con OpenAI GPT-4\n",
        "\n",
        "Integramos ahora la recuperación con GPT-4 para responder preguntas basadas en las noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Configurar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# Crear la función de RAG\n",
        "def rag_query(user_query, top_k=3):\n",
        "    retrieved_docs, metadata = search_news(user_query, top_k)\n",
        "    \n",
        "    # Formatear el contexto para el modelo GPT-4\n",
        "    context = \"\\n\".join(retrieved_docs)\n",
        "    \n",
        "    # Crear el prompt para el modelo\n",
        "    prompt = f\"\"\"\n",
        "    Basado en las siguientes noticias, responde la siguiente pregunta de manera clara y concisa:\n",
        "\n",
        "    Noticias:\n",
        "    {context}\n",
        "\n",
        "    Pregunta: {user_query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Generar la respuesta\n",
        "    response = llm.predict(prompt)\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Prueba de generación\n",
        "user_query = \"¿Cuáles son las últimas noticias sobre el cambio climático?\"\n",
        "response = rag_query(user_query)\n",
        "\n",
        "print(\"\\nRespuesta generada por RAG:\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos GPT-4 para generar respuestas basadas en las noticias recuperadas.\n",
        "rag_query() recupera noticias, formatea un prompt y consulta a GPT-4.\n",
        "Prueba con consulta real: \"¿Cuáles son las últimas noticias sobre el cambio climático?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Evaluación del Sistema RAG\n",
        "\n",
        "Evaluamos la recuperación y generación con Recall@K y ROUGE.\n",
        "\n",
        "## 6.1 Evaluación de la Recuperación con Recall@K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_recall(queries, ground_truths, k=3):\n",
        "    retrieved_texts = [search_news(q, k)[0] for q in queries]\n",
        "    recall_k = sum(any(gt in retrieved for retrieved in retrieved_texts) for gt in ground_truths) / len(queries)\n",
        "    \n",
        "    print(f\"\\nRecall {k}: {recall_k:.2f}\")\n",
        "\n",
        "# Definir consultas y respuestas esperadas\n",
        "test_queries = [\"Noticias sobre economía\", \"Últimos eventos deportivos\"]\n",
        "expected_answers = [\"Economía en crecimiento\", \"Partido de fútbol\"]\n",
        "\n",
        "evaluate_recall(test_queries, expected_answers, k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall@K mide la capacidad del sistema de recuperar información correcta.\n",
        "Si el recall es alto (cercano a 1), la recuperación es precisa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Evaluación de la Generación con ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Comparar respuestas generadas vs. esperadas\n",
        "reference = \"El cambio climático está afectando las temperaturas globales.\"\n",
        "generated = rag_query(\"¿Cómo afecta el cambio climático al planeta?\")\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "scores = scorer.score(reference, generated)\n",
        "\n",
        "print(\"\\nMétricas de Evaluación de Generación:\")\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ROUGE mide la similitud entre la respuesta generada y la respuesta esperada.\n",
        "ROUGE alto indica que el modelo genera respuestas precisas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusiones\n",
        "\n",
        "* Creamos un sistema RAG real con ChromaDB y OpenAI para búsqueda de noticias.\n",
        "* Probamos consultas reales y generamos respuestas con GPT-4.\n",
        "* Evaluamos la recuperación con Recall@K y la generación con ROUGE.\n",
        "\n",
        "RETO: (1) actualización automática de noticias - (2) visualización de embeddings"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab1-nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
