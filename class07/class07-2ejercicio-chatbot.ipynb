{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSUoHE5d2X2T"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Universidad EAFIT \n",
        "# 2025-2\n",
        "# SI7016 - NLP - Lecture 07\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instalar dependencias\n",
        "%pip install openai langchain langchain_community streamlit\n",
        "%pip install faiss-cpu\n",
        "# instalar ChromaDB como retriever \n",
        "%pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# paso 1:\n",
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Configurar la API Key (puedes establecerla como variable de entorno)\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "# bash: export OPENAI_API_KEY=\"your-openai-api-key\"\n",
        "\n",
        "# Inicializar el modelo GPT-4\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar la Base de Datos Vectorial\n",
        "\n",
        "import chromadb\n",
        "\n",
        "# Inicializar la base de datos ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Crear una colección en ChromaDB\n",
        "collection = chroma_client.get_or_create_collection(name=\"chatbot_knowledge\")\n",
        "\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agregar Datos de Referencia a la Base de Datos\n",
        "\n",
        "documents = [\n",
        "    {\"id\": \"1\", \"text\": \"La inteligencia artificial es el campo de la informática que estudia cómo crear sistemas capaces de realizar tareas que requieren inteligencia humana.\"},\n",
        "    {\"id\": \"2\", \"text\": \"GPT-4 es un modelo de lenguaje de OpenAI basado en la arquitectura de transformers.\"},\n",
        "    {\"id\": \"3\", \"text\": \"LangChain es una biblioteca para desarrollar aplicaciones basadas en modelos de lenguaje.\"}\n",
        "]\n",
        "\n",
        "# Agregar documentos a ChromaDB\n",
        "for doc in documents:\n",
        "    collection.add(ids=[doc[\"id\"]], documents=[doc[\"text\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un Retriever con ChromaDB\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Inicializar embeddings con OpenAI\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Crear el VectorStore con ChromaDB\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "\n",
        "# Crear el retriever para buscar documentos relevantes\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# paso 2\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# Crear memoria de la conversación\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Integrar memoria en la cadena conversacional\n",
        "\n",
        "#chat_chain = ConversationalRetrievalChain.from_llm(llm, memory=memory)\n",
        "\n",
        "# fix con un retriever en chromadb\n",
        "chat_chain = ConversationalRetrievalChain.from_llm(llm, memory=memory,retriever=retriever)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paso 3: Construir la Función del Chatbot\n",
        "\n",
        "def chat_with_gpt(user_input):\n",
        "    response = chat_chain({\"question\": user_input})\n",
        "    return response[\"answer\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecución del Chatbot\n",
        "#%pip install streamlit\n",
        "#%pip install tiktoken\n",
        "\n",
        "!streamlit run app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mejoras propuestas\n",
        "\n",
        "# Agregar RAG (Retrieval-Augmented Generation) con bases de datos vectoriales y tus propios datos.\n",
        "# Integrar API externas (ejemplo: Wikipedia, bases de datos de conocimiento).\n",
        "# Incluir Voice-to-Text para hacer un chatbot por voz.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab1-nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
