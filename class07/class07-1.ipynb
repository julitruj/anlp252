{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSUoHE5d2X2T"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Universidad EAFIT \n",
        "# 2025-2\n",
        "# SI7016 - NLP - Lecture 07\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instalar dependencias\n",
        "%pip install transformers\n",
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ejemplos de Lecture07 RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embeddings con HF\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Cargar modelo\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Crear embeddings\n",
        "sentences = [\n",
        "    \"La capital de Francia es París.\",\n",
        "    \"París es la ciudad más importante de Francia.\"\n",
        "]\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(\"Dimensiones:\", embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embeddings con OpenAI\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
        "\n",
        "response = client.embeddings.create(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    input=\"La inteligencia artificial está transformando la educación.\"\n",
        ")\n",
        "\n",
        "vector = response.data[0].embedding\n",
        "print(\"Dimensión:\", len(vector))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FAISS (Facebook AI Similarity Search)\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "d = 768  # dimensión del embedding\n",
        "index = faiss.IndexFlatL2(d)  # índice con distancia euclidiana\n",
        "\n",
        "# Supongamos que tenemos embeddings en un array numpy\n",
        "embeddings = np.random.random((100, d)).astype(\"float32\")\n",
        "index.add(embeddings)\n",
        "\n",
        "query = np.random.random((1, d)).astype(\"float32\")\n",
        "distances, indices = index.search(query, k=5)\n",
        "print(indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chunking y ventanas de contexto\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text = \"Un documento muy largo...\"\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(\"Número de chunks:\", len(chunks))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sparce retrieval - bm25\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "corpus = [\n",
        "    \"La capital de Francia es París\",\n",
        "    \"París es conocida por la Torre Eiffel\",\n",
        "    \"Roma es la capital de Italia\"\n",
        "]\n",
        "\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "query = \"¿Cuál es la capital de Francia?\"\n",
        "tokenized_query = query.split(\" \")\n",
        "scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dense retrieval\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Modelo de embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Corpus\n",
        "docs = [\n",
        "    \"París es la capital de Francia\",\n",
        "    \"La Torre Eiffel está en París\",\n",
        "    \"Roma es la capital de Italia\"\n",
        "]\n",
        "doc_embeddings = model.encode(docs)\n",
        "\n",
        "# Índice vectorial con FAISS\n",
        "d = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(np.array(doc_embeddings))\n",
        "\n",
        "# Consulta\n",
        "query = \"¿Dónde está la Torre Eiffel?\"\n",
        "q_emb = model.encode([query])\n",
        "distances, indices = index.search(np.array(q_emb), k=2)\n",
        "\n",
        "print(\"Documentos recuperados:\", [docs[i] for i in indices[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rerankers\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "pairs = [\n",
        "    (\"¿Cuál es la capital de Francia?\", \"París es la capital de Francia\"),\n",
        "    (\"¿Cuál es la capital de Francia?\", \"Roma es la capital de Italia\")\n",
        "]\n",
        "\n",
        "scores = model.predict(pairs)\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OTROS EJEMPLOS RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ejemplo - Uso de GPT-4 para Q&A Generativo\n",
        "# actualización: https://github.com/openai/openai-python\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"¿Quién descubrió la gravedad?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo: Q&A Mejorado con Recuperación de Información (RAG)\n",
        "# se verá más adelante!!!!\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "# Cargar la base de datos de documentos en ChromaDB\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# Crear un retriever para buscar información en la base de datos\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Crear la Conversational Retrieval Chain con GPT-4\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model_name=\"gpt-4\"),\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Hacer una pregunta con recuperación de documentos\n",
        "query = \"¿Qué es LangChain?\"\n",
        "response = qa_chain({\"question\": query})\n",
        "print(response[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarization - resumen - Ejemplo con T5\n",
        "\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "text = \"Los modelos de lenguaje han cambiado la forma en que interactuamos con la tecnología...\"\n",
        "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generación de texto - Ejemplo con GPT-4\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Escribe un poema sobre la inteligencia artificial.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chatbots y Asistentes Virtuales - Ejemplo con LangChain y GPT-4\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chatbot = ChatOpenAI(model_name=\"gpt-4\")\n",
        "response = chatbot.predict(\"¿Cuáles son los beneficios de la IA?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo: Q&A Mejorado con Recuperación de Información (RAG)\n",
        "# Uso de LangChain con ChromaDB\n",
        "#\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "# Cargar la base de datos de documentos en ChromaDB\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# Crear un retriever para buscar información en la base de datos\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Crear la Conversational Retrieval Chain con GPT-4\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model_name=\"gpt-4\"),\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Hacer una pregunta con recuperación de documentos\n",
        "query = \"¿Qué es LangChain?\"\n",
        "response = qa_chain({\"question\": query})\n",
        "print(response[\"answer\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab1-nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
