{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYI53X4PV6xb"
   },
   "source": [
    "# Modelo de Lenguaje n-gram con KenLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI7003 NLP - SI7016 Applied NLP\n",
    "# Lecture 04 examples\n",
    "# this notebook can run on google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuración en google colab accediendo al Drive (opcional)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy transformers bertviz matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos n-gram y evaluación (repaso)\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# Corpus de ejemplo\n",
    "corpus = \"el gato duerme el gato come el ratón duerme\"\n",
    "tokens = corpus.split()\n",
    "\n",
    "# Unigramas y Bigramas\n",
    "unigrams = Counter(tokens)\n",
    "bigrams = Counter(zip(tokens[:-1], tokens[1:]))\n",
    "\n",
    "# Vocabulario y total\n",
    "V = len(unigrams)\n",
    "N = len(tokens)\n",
    "\n",
    "# Probabilidades MLE\n",
    "P_unigram = {w: c/N for w, c in unigrams.items()}\n",
    "P_bigram = {bg: c/unigrams[bg[0]] for bg, c in bigrams.items()}\n",
    "\n",
    "# Log-likelihood y perplejidad de una frase\n",
    "sentence = \"el gato duerme\"\n",
    "s_toks = sentence.split()\n",
    "logprob = 0.0\n",
    "for i in range(1, len(s_toks)):\n",
    "    bigram = (s_toks[i-1], s_toks[i])\n",
    "    prob = P_bigram.get(bigram, 1/V)\n",
    "    logprob += math.log(prob)\n",
    "perplexity = math.exp(-logprob / (len(s_toks)-1))\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal tipo Bengio 2003\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Toy dataset y vocab\n",
    "sentences = [\"el gato duerme\", \"el gato come\"]\n",
    "vocab = list(set(\" \".join(sentences).split()))\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Datos para entrenamiento\n",
    "X = []\n",
    "y = []\n",
    "for s in sentences:\n",
    "    toks = s.split()\n",
    "    for i in range(1, len(toks)):\n",
    "        X.append(word2idx[toks[i-1]])\n",
    "        y.append(word2idx[toks[i]])\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "# Modelo\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = NNLM(vocab_size, 10, 16)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X)\n",
    "    loss = loss_fn(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Entrenamiento completo. Última pérdida:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo de red neuronal feed forward\n",
    "#!pip install torch matplotlib scikit-learn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Generar datos artificiales\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# 2. Preprocesamiento\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convertir a tensores\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "# 3. Definir  red neuronal feedforward\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = FeedforwardNN()\n",
    "\n",
    "# 4. Definir función de pérdida y optimizador\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 5. Entrenamiento\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 6. Evaluación\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    predictions = (y_pred > 0.5).float()\n",
    "    accuracy = (predictions == y_test_tensor).float().mean()\n",
    "    print(f\"\\nAccuracy on test set: {accuracy:.4f}\")\n",
    "\n",
    "# 7. Visualizar resultados\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions.squeeze(), cmap=\"coolwarm\", alpha=0.7)\n",
    "plt.title(\"Predicciones de la red feedforward\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN vs LSTM\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atención escalar\n",
    "Q = torch.tensor([[0.0, 1.0]])  # Query\n",
    "K = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])  # Keys\n",
    "V = torch.tensor([[1.0, 0.0], [10.0, 0.0], [100.0, 5.0]])  # Values\n",
    "\n",
    "dk = K.size(-1)\n",
    "scores = Q @ K.T / math.sqrt(dk)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "context = weights @ V\n",
    "print(\"Pesos de atención:\", weights)\n",
    "print(\"Contexto resultante:\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paso 1: Crear Q, K y V manualmente\n",
    "Q = np.array([[1, 0, 1, 0],   # query para token 1\n",
    "              [0, 1, 0, 1],   # query para token 2\n",
    "              [1, 1, 1, 1]])  # query para token 3\n",
    "\n",
    "K = np.array([[1, 0, 1, 0],   # key para token 1\n",
    "              [0, 1, 0, 1],   # key para token 2\n",
    "              [1, 1, 0, 0]])  # key para token 3\n",
    "\n",
    "V = np.array([[1, 0, 0, 0],   # value para token 1\n",
    "              [0, 1, 0, 0],   # value para token 2\n",
    "              [0, 0, 1, 0]])  # value para token 3\n",
    "\n",
    "# Paso 2: Calcular los scores (QK^T)\n",
    "scores = Q @ K.T\n",
    "print(\"Raw scores (QK^T):\\n\", scores)\n",
    "\n",
    "# Paso 3: Escalar por sqrt(d_k) (aquí d_k = 4)\n",
    "dk = Q.shape[1]\n",
    "scaled_scores = scores / np.sqrt(dk)\n",
    "print(\"\\nScaled scores:\\n\", scaled_scores)\n",
    "\n",
    "# Paso 4: Aplicar softmax fila por fila\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # estabilidad numérica\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "print(\"\\nAttention Weights (softmax):\\n\", attention_weights)\n",
    "\n",
    "# Paso 5: Multiplicar por V (ponderar los valores)\n",
    "output = attention_weights @ V\n",
    "print(\"\\nOutput (Attention weighted values):\\n\", output)\n",
    "\n",
    "# Paso 6: Visualizar la matriz de atención\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(attention_weights, annot=True, cmap='Blues', xticklabels=['Tok1', 'Tok2', 'Tok3'], yticklabels=['Q1', 'Q2', 'Q3'])\n",
    "plt.title('Matriz de Atención (peso de cada token sobre los valores)')\n",
    "plt.xlabel('Keys (K)')\n",
    "plt.ylabel('Queries (Q)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: Visualización de atención con BERT y BERTViz (modelo en inglés)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertviz import head_view, model_view\n",
    "import torch\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 1: Cargar modelo BERT en inglés\n",
    "# ------------------------------\n",
    "model_name = 'bert-base-uncased'  # Modelo entrenado para inglés\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval();\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 2: Tokenizar oración de ejemplo\n",
    "# ------------------------------\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "inputs = tokenizer.encode_plus(sentence, return_tensors='pt', add_special_tokens=True)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Obtener tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 3: Ejecutar modelo y obtener atención\n",
    "# ------------------------------\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    attention = outputs.attentions  # Tuple con 12 tensores (capas), cada uno de forma (1, n_heads, seq_len, seq_len)\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 4: Visualización con BERTViz\n",
    "# ------------------------------\n",
    "# Vista por cabeza\n",
    "head_view(attention, tokens=tokens)\n",
    "\n",
    "# Vista por capa (opcional)\n",
    "# model_view(attention, tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: Visualización de atención con BERT y BERTViz (modelo en español)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertviz import head_view, model_view\n",
    "import torch\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 1: Cargar modelo BERT en español\n",
    "# ------------------------------\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'  # Modelo entrenado para español\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval();\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 2: Tokenizar oración de ejemplo\n",
    "# ------------------------------\n",
    "sentence = \"El gato se sentó sobre la alfombra.\"\n",
    "inputs = tokenizer.encode_plus(sentence, return_tensors='pt', add_special_tokens=True)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Obtener tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 3: Ejecutar modelo y obtener atención\n",
    "# ------------------------------\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    attention = outputs.attentions  # Tuple con 12 tensores (capas), cada uno de forma (1, n_heads, seq_len, seq_len)\n",
    "\n",
    "# ------------------------------\n",
    "# Paso 4: Visualización con BERTViz\n",
    "# ------------------------------\n",
    "# Vista por cabeza\n",
    "head_view(attention, tokens=tokens)\n",
    "\n",
    "# Vista por capa (opcional)\n",
    "# model_view(attention, tokens=tokens)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
