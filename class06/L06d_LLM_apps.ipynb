{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n",
        "# Universidad EAFIT \n",
        "# 2025-2\n",
        "# SI7016 - NLP - Lecture 06 - LLM apps\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instalar dependencias\n",
        "%pip install transformers\n",
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "# tu token\n",
        "login(\"hf_xxxxxxxxxxxxx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWBFHbzS20OQ",
        "outputId": "6688528e-4d45-4279-9066-12dc0ed9f90f"
      },
      "outputs": [],
      "source": [
        "# clasificación de texto\n",
        "# modelo preentrenado de HF para clasificar reseñas de clientes en positivas o negativas:\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "result = classifier(\"Este producto es increíble, me encantó.\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analisis de sentimientos\n",
        "\n",
        "from transformers import pipeline\n",
        "analyzer = pipeline(\"sentiment-analysis\")\n",
        "analyzer(\"No me gustó el servicio, fue una experiencia terrible.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo 1 con BERT-QA\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "context = \"El modelo de lenguaje BERT fue desarrollado por Google AI en 2018.\"\n",
        "question = \"¿Quién desarrolló BERT?\"\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "print(result['answer'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ejemplo 2 - Uso de GPT-4 para Q&A Generativo\n",
        "# actualización: https://github.com/openai/openai-python\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"¿Quién descubrió la gravedad?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo 3: Q&A Mejorado con Recuperación de Información (RAG)\n",
        "# se verá más adelante!!!!\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "# Cargar la base de datos de documentos en ChromaDB\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# Crear un retriever para buscar información en la base de datos\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Crear la Conversational Retrieval Chain con GPT-4\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model_name=\"gpt-4\"),\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Hacer una pregunta con recuperación de documentos\n",
        "query = \"¿Qué es LangChain?\"\n",
        "response = qa_chain({\"question\": query})\n",
        "print(response[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarization - resumen - Ejemplo con T5\n",
        "\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "text = \"Los modelos de lenguaje han cambiado la forma en que interactuamos con la tecnología...\"\n",
        "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generación de texto - Ejemplo con GPT-4\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Escribe un poema sobre la inteligencia artificial.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chatbots y Asistentes Virtuales - Ejemplo con LangChain y GPT-4\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chatbot = ChatOpenAI(model_name=\"gpt-4\")\n",
        "response = chatbot.predict(\"¿Cuáles son los beneficios de la IA?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo 3: Q&A Mejorado con Recuperación de Información (RAG)\n",
        "# Uso de LangChain con ChromaDB\n",
        "#\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "# Cargar la base de datos de documentos en ChromaDB\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# Crear un retriever para buscar información en la base de datos\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Crear la Conversational Retrieval Chain con GPT-4\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model_name=\"gpt-4\"),\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Hacer una pregunta con recuperación de documentos\n",
        "query = \"¿Qué es LangChain?\"\n",
        "response = qa_chain({\"question\": query})\n",
        "print(response[\"answer\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab1-nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
