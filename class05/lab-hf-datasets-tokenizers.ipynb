{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15878,
     "status": "ok",
     "timestamp": 1754959124231,
     "user": {
      "displayName": "Edwin Nelson Montoya",
      "userId": "10708817807819244709"
     },
     "user_tz": 300
    },
    "id": "S9tXo10QkT6B",
    "outputId": "c3e73042-acdd-4550-c9db-13c89358c400"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C11vykvjkmPO"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVEQop-3kmSy"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers accelerate sentencepiece  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets en hugging face\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceder a datos:\n",
    "\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reto: mirar el elemwento 15 del training set\n",
    "# mirar el elemento 87 del validation set.\n",
    "\n",
    "print(raw_train_dataset[15])\n",
    "raw_val_dataset = raw_datasets[\"validation\"]\n",
    "print(raw_val_dataset[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenización a nivel de oración\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reto \n",
    "# extraer el elemento 15 del training set y tokenicelo como sentencias separadas y como una pareja\n",
    "# Cual es la diferencia entre los 2 resultados\n",
    "\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "stn1 = raw_train_dataset[15]['sentence1']\n",
    "stn2 = raw_train_dataset[15]['sentence2']\n",
    "print(stn1)\n",
    "print(stn2)\n",
    "inputs = tokenizer(stn1, stn2,stn1+stn2)\n",
    "print(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de ids a tokens de nuevo\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizar todo un dataset completo\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix padding\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function,batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['idx','sentence1','sentence2'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label','labels')\n",
    "tokenized_dataset = tokenized_dataset.with_format('torch')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloaders = DataLoader(tokenized_dataset['train'],batch_size=16,shuffle=True)\n",
    "\n",
    "for step,batch in enumerate(train_dataloaders):\n",
    "    print(batch['input_ids'].shape)\n",
    "    if (step > 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic padding\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function,batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['idx','sentence1','sentence2'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label','labels')\n",
    "tokenized_dataset = tokenized_dataset.with_format('torch')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collactor = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_dataloaders = DataLoader(tokenized_dataset['train'],batch_size=16,shuffle=True,collate_fn=data_collactor)\n",
    "\n",
    "for step,batch in enumerate(train_dataloaders):\n",
    "    print(batch['input_ids'].shape)\n",
    "    if (step > 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_dataset[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOxeerSK371aR0Pcd32DECr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
