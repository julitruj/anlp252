{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde00888",
   "metadata": {},
   "source": [
    "# Lab BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar las librerias\n",
    "# google colab ya las tiene\n",
    "!pip install -U transformers datasets evaluate accelerate bertviz torch torchvision torchaudio -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75077a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta no viene con google colab\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce9a90",
   "metadata": {},
   "source": [
    "## Imports y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    BertTokenizerFast, BertModel, BertForSequenceClassification,\n",
    "    pipeline, AutoTokenizer, AutoModel, AutoModelForQuestionAnswering\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def cosine_sim(a, b, eps=1e-8):\n",
    "    a = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)\n",
    "    b = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)\n",
    "    return (a * b).sum(axis=-1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1ab4f",
   "metadata": {},
   "source": [
    "## Tokenización WordPiece y embeddings contextuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"El banco está lleno de gente. El banco está junto al río.\"\n",
    "tok = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device).eval()\n",
    "\n",
    "enc = tok(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model(**{k: v.to(device) for k, v in enc.items()})\n",
    "last_hidden = out.last_hidden_state[0]  # [seq_len, hidden]\n",
    "tokens = tok.convert_ids_to_tokens(enc[\"input_ids\"][0])\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Shape embeddings:\", last_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3c47f",
   "metadata": {},
   "source": [
    "**Reto:** compara los embeddings del primer token `banco` en dos oraciones distintas (ambigüedad semántica).  \n",
    "\n",
    "tip: segmenta el texto en dos inputs y compara los 2 vectores de `banco` con coseno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent1 = \"El banco está lleno de gente.\"\n",
    "#sent2 = \"El banco está junto al río.\"\n",
    "#sent2 = \"El banco de carpinteria esta sucio.\"\n",
    "sent2 = \"El banco de jugadores reserva del DIM es muy bueno.\"\n",
    "\n",
    "\n",
    "enc1 = tok(sent1, return_tensors=\"pt\")\n",
    "enc2 = tok(sent2, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    h1 = model(**{k: v.to(device) for k, v in enc1.items()}).last_hidden_state[0].cpu().numpy()\n",
    "    h2 = model(**{k: v.to(device) for k, v in enc2.items()}).last_hidden_state[0].cpu().numpy()\n",
    "\n",
    "toks1 = tok.convert_ids_to_tokens(enc1[\"input_ids\"][0])\n",
    "toks2 = tok.convert_ids_to_tokens(enc2[\"input_ids\"][0])\n",
    "\n",
    "# Busca índice de 'banco' (puede tokenizarse en subpalabras; aquí asumimos token completo en mBERT)\n",
    "idx1 = toks1.index(\"banco\") if \"banco\" in toks1 else 0\n",
    "idx2 = toks2.index(\"banco\") if \"banco\" in toks2 else 0\n",
    "\n",
    "sim = cosine_sim(h1[idx1:idx1+1], h2[idx2:idx2+1])[0]\n",
    "print(\"Token sent1:\", toks1[idx1], \"| Token sent2:\", toks2[idx2])\n",
    "print(\"Similitud coseno entre 'banco' (contextos distintos):\", float(sim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2adb9",
   "metadata": {},
   "source": [
    "## Clasificación de textos (análisis de sentimientos) con `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "ejemplos = [\n",
    "    \"Este restaurante fue fantástico, volveré pronto.\",\n",
    "    \"El servicio fue terrible y la comida fría.\"\n",
    "]\n",
    "clf(ejemplos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb0c16",
   "metadata": {},
   "source": [
    "## NER (Reconocimiento de Entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_en = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "texto_en = \"Barack Obama was born in Hawaii and served as President of the United States.\"\n",
    "ner_en(texto_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debf0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER en español\n",
    "# Otros modelos en HF: \"PlanTL-GOB-ES/roberta-base-bne-capitel-ner\"\n",
    "ner_es = pipeline(\"token-classification\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n",
    "texto_es = \"Shakira nació en Barranquilla y es una cantante colombiana.\"\n",
    "ner_es(texto_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6990fd2",
   "metadata": {},
   "source": [
    "## Pregunta–Respuesta (SQuAD-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline(\"question-answering\", model=\"deepset/bert-base-cased-squad2\")\n",
    "contexto = \"\"\"\n",
    "BERT es un modelo de lenguaje basado en Transformers desarrollado por Google AI en 2018.\n",
    "Fue entrenado en Wikipedia y BookCorpus, y ha alcanzado resultados de estado del arte en múltiples benchmarks.\n",
    "\"\"\"\n",
    "qa({\"question\": \"¿Quién desarrolló BERT?\", \"context\": contexto})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860046d0",
   "metadata": {},
   "source": [
    "## Similitud de oraciones (embeddings BERT + coseno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d274b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tok_en = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "mdl_en = BertModel.from_pretrained(\"bert-base-uncased\").to(device).eval()\n",
    "\n",
    "sentences = [\n",
    "    \"The cat sits on the mat.\",\n",
    "    \"A feline is resting on a rug.\",\n",
    "    \"We are training a neural network.\",\n",
    "]\n",
    "\n",
    "def cls_embed(sentence):\n",
    "    enc = tok_en(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        h = mdl_en(**{k: v.to(device) for k, v in enc.items()}).last_hidden_state[:, 0, :]\n",
    "    return h[0].cpu().numpy()\n",
    "\n",
    "embs = np.stack([cls_embed(s) for s in sentences])\n",
    "S = cosine_sim(embs[:, None, :], embs[None, :, :])\n",
    "print(\"Matriz de similitud (coseno):\\n\", np.round(S, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee2a7a",
   "metadata": {},
   "source": [
    "## Fine-tuning en clasificación (SST-2, subconjunto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382cd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import evaluate\n",
    "import random\n",
    "\n",
    "# Carga SST2 y reduce tamaño para demo rápida\n",
    "ds = load_dataset(\"glue\", \"sst2\")\n",
    "small_train = ds[\"train\"].shuffle(seed=42).select(range(200))   # 200 ejemplos\n",
    "small_val = ds[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize(ex):\n",
    "    return tokenizer(ex[\"sentence\"], truncation=True)\n",
    "\n",
    "small_train = small_train.map(tokenize, batched=True)\n",
    "small_val = small_val.map(tokenize, batched=True)\n",
    "\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels).to(device)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"glue\", \"sst2\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (preds == labels).mean()}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert-sst2-demo\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_res = trainer.evaluate()\n",
    "eval_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT example:\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Cargar modelo y tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"BERT is amazing for NLP\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Obtener embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "cls_embedding = last_hidden_state[:,0,:]  # [CLS] token\n",
    "print(cls_embedding.shape)  # (1, 768)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
